[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I am a self employed data scientist/statistician/software engineer. When I’m not working I enjoy bouldering, swimming, cycling and hiking. I live in Bristol.\nYou can contact me by email: david @ mawds.co.uk\nYou can see my full career history on Linkedin, or in my CV, but briefly:"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About me",
    "section": "Experience",
    "text": "Experience\nSelf Employed | tupledata limited | September 2023 - Present\nTravelling | Europe | Summer 2023\nSenior Research Software Engineer | University of Manchester | June 2021 - June 2023\nTechnical Director | Vuzo Ltd | April 2019 - May 2021\nInterim Infrastructure Director | DataHE | Febuary 2019 - March 2019\nResearch Software Engineer | University of Manchester | November 2016 - January 2019\nResearch Assistant -&gt; Senior Research Associate | University of Bristol | September 2013 - January 2015\nAnalyst -&gt; Senior Analyst | Higher Education Funding Council for England | July 2007 to September 2013"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\nMSc Medical Statistics | University of Leicester | October 2013 - September 2014\nPhD Department of Physics | University of Bath | October 2003 - January 2007\nMPhys Physics with Computational Physics | University of Sussex | October 1999 to June 2003"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "David Mawdsley’s Blog",
    "section": "",
    "text": "Ubuntu 24.04 upgrade\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLocal fuel prices\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTravels\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAdding epubs to Kobo from my phone\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNot so smart metering\n\n\n\n\n\n\nPython\n\n\nRaspberry Pi\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEasily deploying Shiny apps\n\n\n\n\n\n\nShiny\n\n\nR\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/shinysender/shinysender.html",
    "href": "posts/shinysender/shinysender.html",
    "title": "Easily deploying Shiny apps",
    "section": "",
    "text": "I’m currently running a pilot Shiny web hosting service at the University of Manchester. One issue with the standalone (free) Shiny Server is that it’s not particularly easy to deploy apps to it. Essentially you have to copy them to the server (e.g. using scp), and sort out any library dependency issues. RStudio’s Posit’s commercial offerings, like RStudio Posit Connect make this process much easier, but are more expensive1.\nTo fill this gap, I wrote an R Package to simplify the deployment of apps to a standalone, open source Shiny Server: Shinysender\n\n\nI designed the package to be as simple to use as possible, to minimise the amount of support required.\nThe server being deployed to is set up in the usual way. A few R packages (shiny, rmarkdown, packrat and devtools) need to be installed for all users, and the server needs to be configured to serve apps from users’ home directories. It’s also sensible to pre-install the “dev” versions of of system libraries that are required by common R packages (e.g. libssl-dev, libxml2-dev etc.; the full list of the ones I’ve used is in the README)\nFull details are provided in the package’s readme. Users need an account on the server to be able to deploy to it.\n\n\n\nThe package isn’t (yet) on CRAN, so it needs to be installed from github:\n\ninstall.packages(\"devtools\")  # If you don't already have devtools installed\ndevtools::install_github(\"UoMResearchIT/r-shinysender\")\n\nThe server and username are determined by the SHINYSENDER_SERVER and SHINYSENDER_USER environment variables. These can be set from the R console, or by adding them to ~/.Rprofile/~/.Renviron:\n\nSys.setenv(SHINYSENDER_SERVER=\"myshinyserver.mawds.co.uk\")  \nSys.setenv(SHINYSENDER_USER=\"david\")\n\nThe “upload app” addin will appear in RStudio, and will deploy the current app to http://myshinyserver.mawds.co.uk/username/appname The user is prompted for their password on the server (an ssh key will be used if available).\nThe user’s environment is replicated as closely as possible using Packrat.\nThere are various options to allow the user to set the name of the app on the server, etc. These are detailed in the readme.\n\n\n\nWe use the ssh library to drive the remote session.\nThe app deployment process checks that the users ShinyApps directory exists and creates it if required. We also create as ShinyApps_staging directory.\nWe then use the rsconnect library to prepare a “bundle” containing the app’s code and dependencies. The bundle is then uploaded over scp to the staging directory and decompressed\nWe then upload a custom .Rprofile which sets up the cache directory for Packrat (so that a user can share R packages between their apps). This doesn’t actually activate Packrat (see below). We then restore the libraries into the Packrat environment, before swapping the .Rprofile for one that actually activates Packrat when the app is run.\nAssuming everything has worked, we then move the app directory from ShinyApps_staging to ShinyApps, to make it live.\n\n\nOne feature I was keen on implementing was being able to install custom libraries from Github. These might be stored in a private repository. To handle this, we set any local GITHUB_PAT on the remote before we restore the R packages (We do this as a single command, with history turned off, so the PAT is never actually stored on the server2).\nIf we use the same .Rprofile for deployment and staging, we won’t have access to the (system-wide) devtools library, which is required to install packages from github. Hence the need for a staging .Rprofile and a live one.\nI should add that our .Rprofile is appended to any existing .Rprofile, rather than being overwritten, so any settings the user has used will be preserved.\n\n\n\n\nShiny server doesn’t let you use more than one version of R. Although we attempt to replicate the user’s environment as closely as possible, this does mean that they’re stuck with the version of R on the server. In practice, this hasn’t proved an issue so far, but is likely to become problematic when R’s major version increments.\nThe usual limitations of hosting apps on the free version of Shiny server (i.e. an app only has a single process, which is shared by all users applies). On my aspirational todo list is to deploy apps using Shinyproxy - this would mean each app was hosted in its own Docker container. In principle that should be fairly straightforward, since the packrat steps are generic. From a security perspective it’s more difficult, since the shinyproxy config would need to be modified by a privileged user to pick up the new app. Essentially I’d need to write an API for the server to do this, rather than just “driving” a remote ssh session as the user.\nIt’d be better to run the whole thing behind an API, but since the Shiny apps are run as the user, and are deployed using the user account the security risks are much lower. The server is locked down so users cannot see others’ home directories etc."
  },
  {
    "objectID": "posts/shinysender/shinysender.html#usage---server-setup",
    "href": "posts/shinysender/shinysender.html#usage---server-setup",
    "title": "Easily deploying Shiny apps",
    "section": "",
    "text": "I designed the package to be as simple to use as possible, to minimise the amount of support required.\nThe server being deployed to is set up in the usual way. A few R packages (shiny, rmarkdown, packrat and devtools) need to be installed for all users, and the server needs to be configured to serve apps from users’ home directories. It’s also sensible to pre-install the “dev” versions of of system libraries that are required by common R packages (e.g. libssl-dev, libxml2-dev etc.; the full list of the ones I’ve used is in the README)\nFull details are provided in the package’s readme. Users need an account on the server to be able to deploy to it."
  },
  {
    "objectID": "posts/shinysender/shinysender.html#usage---app-deployment",
    "href": "posts/shinysender/shinysender.html#usage---app-deployment",
    "title": "Easily deploying Shiny apps",
    "section": "",
    "text": "The package isn’t (yet) on CRAN, so it needs to be installed from github:\n\ninstall.packages(\"devtools\")  # If you don't already have devtools installed\ndevtools::install_github(\"UoMResearchIT/r-shinysender\")\n\nThe server and username are determined by the SHINYSENDER_SERVER and SHINYSENDER_USER environment variables. These can be set from the R console, or by adding them to ~/.Rprofile/~/.Renviron:\n\nSys.setenv(SHINYSENDER_SERVER=\"myshinyserver.mawds.co.uk\")  \nSys.setenv(SHINYSENDER_USER=\"david\")\n\nThe “upload app” addin will appear in RStudio, and will deploy the current app to http://myshinyserver.mawds.co.uk/username/appname The user is prompted for their password on the server (an ssh key will be used if available).\nThe user’s environment is replicated as closely as possible using Packrat.\nThere are various options to allow the user to set the name of the app on the server, etc. These are detailed in the readme."
  },
  {
    "objectID": "posts/shinysender/shinysender.html#how-it-works",
    "href": "posts/shinysender/shinysender.html#how-it-works",
    "title": "Easily deploying Shiny apps",
    "section": "",
    "text": "We use the ssh library to drive the remote session.\nThe app deployment process checks that the users ShinyApps directory exists and creates it if required. We also create as ShinyApps_staging directory.\nWe then use the rsconnect library to prepare a “bundle” containing the app’s code and dependencies. The bundle is then uploaded over scp to the staging directory and decompressed\nWe then upload a custom .Rprofile which sets up the cache directory for Packrat (so that a user can share R packages between their apps). This doesn’t actually activate Packrat (see below). We then restore the libraries into the Packrat environment, before swapping the .Rprofile for one that actually activates Packrat when the app is run.\nAssuming everything has worked, we then move the app directory from ShinyApps_staging to ShinyApps, to make it live.\n\n\nOne feature I was keen on implementing was being able to install custom libraries from Github. These might be stored in a private repository. To handle this, we set any local GITHUB_PAT on the remote before we restore the R packages (We do this as a single command, with history turned off, so the PAT is never actually stored on the server2).\nIf we use the same .Rprofile for deployment and staging, we won’t have access to the (system-wide) devtools library, which is required to install packages from github. Hence the need for a staging .Rprofile and a live one.\nI should add that our .Rprofile is appended to any existing .Rprofile, rather than being overwritten, so any settings the user has used will be preserved."
  },
  {
    "objectID": "posts/shinysender/shinysender.html#limitations",
    "href": "posts/shinysender/shinysender.html#limitations",
    "title": "Easily deploying Shiny apps",
    "section": "",
    "text": "Shiny server doesn’t let you use more than one version of R. Although we attempt to replicate the user’s environment as closely as possible, this does mean that they’re stuck with the version of R on the server. In practice, this hasn’t proved an issue so far, but is likely to become problematic when R’s major version increments.\nThe usual limitations of hosting apps on the free version of Shiny server (i.e. an app only has a single process, which is shared by all users applies). On my aspirational todo list is to deploy apps using Shinyproxy - this would mean each app was hosted in its own Docker container. In principle that should be fairly straightforward, since the packrat steps are generic. From a security perspective it’s more difficult, since the shinyproxy config would need to be modified by a privileged user to pick up the new app. Essentially I’d need to write an API for the server to do this, rather than just “driving” a remote ssh session as the user.\nIt’d be better to run the whole thing behind an API, but since the Shiny apps are run as the user, and are deployed using the user account the security risks are much lower. The server is locked down so users cannot see others’ home directories etc."
  },
  {
    "objectID": "posts/shinysender/shinysender.html#footnotes",
    "href": "posts/shinysender/shinysender.html#footnotes",
    "title": "Easily deploying Shiny apps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey also offer other features, like making the app more scalable if it has many concurrent users.↩︎\nThis was the most secure way I could think of doing this..if there are better ways I’d be grateful to hear of them.↩︎"
  },
  {
    "objectID": "posts/fuelprice/fuelprice.html",
    "href": "posts/fuelprice/fuelprice.html",
    "title": "Local fuel prices",
    "section": "",
    "text": "I’ve recently launched a new site localfuelprices.co.uk to display the cost of petrol and diesel nearby petrol stations.\nIt’s built using django and geodjango, with leaflet for the mapping."
  },
  {
    "objectID": "posts/travels/travels.html",
    "href": "posts/travels/travels.html",
    "title": "Travels",
    "section": "",
    "text": "I’m taking a few months career break to go travelling, which I plan to blog (hopefully more regularly than I manage here!) at https://travel.mawds.co.uk/index.php/europe-2023/. This is running on Wordpress, which is a lot easier to update on a phone than this Quarto based site."
  },
  {
    "objectID": "posts/powermon/powermon.html",
    "href": "posts/powermon/powermon.html",
    "title": "Not so smart metering",
    "section": "",
    "text": "This project has been a while in the making, and I’ve finally got round to finishing it off (well as much as anything is finished). A big part of the delay was, somewhat ireonically, not having a power socket near the meters.\nI’m not a huge fan of Smart Meters - being able to be cut off remotely doesn’t seem a selling point, but I was curious how much energy I was using. I built a circuit with the Raspberry Pi to keep track of much gas and electricity I was using - this uses the GPIO pins to count the flashes on the electricity meter, using an LDR, and to count the rotations of the gas meter using a Hall probe (it turns out the 0 has a magnet in it, which can be detected as it rotates).\nThe measurements are much more precise for the electic meter - there are 1000 flashes per kWh, compared to ~9 per kWh for the gas meter.\nI was originally simply dumping the timestamps (and time difference between successive ones for each power source) into a CSV file, and looking at this in R.\nI wanted to make something where I could see at a glance how much energy I’d been using. I had an old Nook E-Reader, which I planed to use as a display. This blog post was very useful in figuring out how to do this.\ntldr; it runs Android 2.x, the apks linked to on that page were sideloaded via adb, so no need to set up a Google Account. The plan was to use Electic Sign to display a web page containing the current power usage.\n\n\nI’ve now switched to dumping the timestamps to a sqllite database, and am using FastAPI to generate an endpoint that I can display on the Nook:\n\n\n\n\nI plan to move the logging over to a “proper” database. Handling timestamps in Sqllite is a little fiddly. Doing some visualisation of the data is also on the list.\nCode is here"
  },
  {
    "objectID": "posts/powermon/powermon.html#sqllite-and-fastapi",
    "href": "posts/powermon/powermon.html#sqllite-and-fastapi",
    "title": "Not so smart metering",
    "section": "",
    "text": "I’ve now switched to dumping the timestamps to a sqllite database, and am using FastAPI to generate an endpoint that I can display on the Nook:"
  },
  {
    "objectID": "posts/powermon/powermon.html#futher-work",
    "href": "posts/powermon/powermon.html#futher-work",
    "title": "Not so smart metering",
    "section": "",
    "text": "I plan to move the logging over to a “proper” database. Handling timestamps in Sqllite is a little fiddly. Doing some visualisation of the data is also on the list.\nCode is here"
  },
  {
    "objectID": "posts/ubuntu_reinstall/ubuntu_reinstall.html",
    "href": "posts/ubuntu_reinstall/ubuntu_reinstall.html",
    "title": "Ubuntu 24.04 upgrade",
    "section": "",
    "text": "I recently had to reinstall Ubuntu on my ageing Thinkpad X260, after the upgrade from 22.04 to 24.04 went horribly wrong.\nI’m still not quite sure what went wrong. It think I’d somehow ended up with (only) the 22.04 kernel installed, which led to lots of “Unknown symbol” errors. My system was quite heavily customised, so it was perhaps a bit optimistic to expect the upgrade to work.\nThis post is mostly a note to self about how I reconfigured the dual boot setup in case I need to do it again (which I hope I don’t).\nI initially thought it was only the kernel that was the problem. This stackexchange post was really useful for mounting the encrypted drive, setting up a chroot environment and reinstalling the kernel. That got me booting to the command prompt.\nIt turned out I was missing most packages, at which point I decided to wipe and restore from backups.\n(I could actually access my encrypted drive through the Ubuntu LiveCDUSB, so took a fresh backup of /home and /etc from this)\nI’d got an (essentially unused and tiny) Windows 10 partition dual booting with Ubuntu. I was keen to keep Windows 10 on the machine, as it’s occasionally useful. I’d previously set this up to dual boot with full disk encryption for both OSs roughly following the guide here, but using the legacy BIOS, as that’s what Windows 10 had come preinstalled with.\nI was pleased to see that the guide had been updated for Ubuntu 24.04 and is now much easier. I took this opportunity to wipe the Windows partition and switch to UEFI too.\nThe basic process was:\n\nUse Ubuntu 24.04 USB image to check backups are OK and work\nUse the Ubuntu USB image and gparted to remove all partitions\nReboot and set the BIOS to UEFI mode (only) (Startup menu in BIOS), and check the security chip is set to “Intel PTT”.\nInstall Windows 10 Pro from installation USB (avoiding creating a Microsoft account by selecting “Work or school account” then “Domain join instead” via\nInstall interminable updates and reboot lots of times (I kept force checking for updates until there were no more). (Bitlocker is not enabled at this point)\nReboot with the Ubuntu USB image\nFollow the instructions at https://www.mikekasberg.com/blog/2024/05/20/dual-boot-ubuntu-24-04-and-windows-with-encryption.html to setup the dual boot with an encrypted partition setup.\nCheck both still boot\nEnable bitlocker on the Windows partition You need to boot Windows via the BIOS’s boot menu (F12 on my machine), not via Grub, otherwise it will prompt you for the bitlocker recovery key on boot (details of how I removed the Windows option from Grub below)\n\nOnce everything was working, I rebooted using the USB image to mount my nfs share containing the backups and restored my data.\nA few post install tweaks:\nHibernate doesn’t work out of the box. I enabled this following the guide on Elbrar’s corner. This requires booting with the USB image again, to resize volumes to make space for the encrypted swap partition (with hindsight, the resizing would probably have been quicker if I did this before restoring my data; even so, it was only a few minutes). I didn’t follow the “Install Clevis” section, as I prefer to enter the decryption key before the machine boots. I used the Hibernate Status Button extension to enable the hibernate menu option on the desktop.\nSuspend didn’t work properly. The system would suspend, but not wake up. I eventually solved this via this Stackexchange post. You need to add intel_iommu=off to the GRUB_CMDLINE_LINUX_DEFAULT options in /etc/default/grub and then update-grub.\nI also took this opportunity to add DISABLE_OS_PROBER=true to the file, so we don’t have the option to boot to Windows from grub, which causes issues with Bitlocker, as mentioned above.\nI also had to force an update of the installed Snaps so that Firefox was at the same version as on the 20.04 system, so that it would load my profile data. Presumably this would’ve happend automatically at some point anyway."
  },
  {
    "objectID": "posts/kobo/kobo.html",
    "href": "posts/kobo/kobo.html",
    "title": "Adding epubs to Kobo from my phone",
    "section": "",
    "text": "I’ve got a Kobo e-reader, which is very fussy about the USB cable used to copy books to it (and even the USB port the cable is connected to). I want to be able to sideload books from, e.g. Project Gutenberg to it from my phone. The (beta) web browser that’s included with the Kobo is very basic, and I’ve struggled to get it to work with most web sites, so the obvious solution doesn’t really work.\nWith thanks to this post, I figured out a way to do this.\nEssentially you download your books using Chrome on the phone, and then use termux, which give a Linux-like environment on the phone, to run a minimal web-server whose output works with the Kobo’s browser."
  },
  {
    "objectID": "posts/kobo/kobo.html#setup",
    "href": "posts/kobo/kobo.html#setup",
    "title": "Adding epubs to Kobo from my phone",
    "section": "Setup",
    "text": "Setup\nAt the termux prompt: (this only needs to be done once)\ntermux-setup-storage # Gives access to the Download folder on android \npkg install python3 # Install Python"
  },
  {
    "objectID": "posts/kobo/kobo.html#to-use",
    "href": "posts/kobo/kobo.html#to-use",
    "title": "Adding epubs to Kobo from my phone",
    "section": "To use",
    "text": "To use\nEnable tethering on the phone (I also found I had to disable the VPN running on the phone)\nAt the termux prompt:\nifconfig # Note the ip associated with wlan0\ncd ~/storage/downloads # Assuming this is where the book is\npython -m http.server 8080\nOn the Kobo, connect to the phone’s hotspot, open the browser (more, beta features), and navigate to http://&lt;phone’s ip&gt;:8080 This gives a very basic directory listing, which works OK on the Kobo"
  }
]